{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "リスト内の1の和  \n",
    "e.g.  \n",
    "[1,1,0,0,0,0,1,0,0,0] -> 3  \n",
    "\n",
    "http://peterroelants.github.io/posts/rnn_implementation_part01/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テストデータ数:  300\n",
      "トレーニングデータ数:  700\n",
      "train#0, loss: 218.46011352539062\n",
      "0.187143\n",
      "train#100, loss: 100.57823181152344\n",
      "train#200, loss: 112.99855041503906\n",
      "train#300, loss: 20.311420440673828\n",
      "train#400, loss: 11.360490798950195\n",
      "train#500, loss: 6.569569110870361\n",
      "0.98\n",
      "train#600, loss: 3.9332756996154785\n",
      "train#700, loss: 4.560934066772461\n",
      "train#800, loss: 4.625699520111084\n",
      "train#900, loss: 3.8007025718688965\n",
      "train#1000, loss: 1.9310370683670044\n",
      "0.991429\n",
      "train#1100, loss: 1.556487798690796\n",
      "train#1200, loss: 0.9503235220909119\n",
      "train#1300, loss: 1.6706324815750122\n",
      "train#1400, loss: 2.4234113693237305\n",
      "train#1500, loss: 1.1390825510025024\n",
      "0.997143\n",
      "train#1600, loss: 0.8369040489196777\n",
      "train#1700, loss: 0.6445013284683228\n",
      "train#1800, loss: 1.3029805421829224\n",
      "train#1900, loss: 0.8373410105705261\n",
      "train#2000, loss: 0.4186079502105713\n",
      "0.997143\n",
      "train#2100, loss: 0.2931709587574005\n",
      "train#2200, loss: 0.34993115067481995\n",
      "train#2300, loss: 0.8678434491157532\n",
      "train#2400, loss: 0.7747615575790405\n",
      "train#2500, loss: 0.8869129419326782\n",
      "1.0\n",
      "train#2600, loss: 0.40794339776039124\n",
      "train#2700, loss: 0.7470126152038574\n",
      "train#2800, loss: 0.45476001501083374\n",
      "train#2900, loss: 7.0829877853393555\n",
      "train#3000, loss: 0.41505879163742065\n",
      "1.0\n",
      "train#3100, loss: 0.38558483123779297\n",
      "train#3200, loss: 0.37535813450813293\n",
      "train#3300, loss: 0.7799243927001953\n",
      "train#3400, loss: 0.09403746575117111\n",
      "train#3500, loss: 0.1342354267835617\n",
      "1.0\n",
      "train#3600, loss: 0.5016093254089355\n",
      "train#3700, loss: 0.3119811415672302\n",
      "train#3800, loss: 0.35510629415512085\n",
      "train#3900, loss: 0.23933912813663483\n",
      "train#4000, loss: 0.23050954937934875\n",
      "1.0\n",
      "train#4100, loss: 0.19384053349494934\n",
      "train#4200, loss: 0.16000621020793915\n",
      "train#4300, loss: 0.17043474316596985\n",
      "train#4400, loss: 0.20515263080596924\n",
      "train#4500, loss: 0.1528923511505127\n",
      "1.0\n",
      "train#4600, loss: 0.12030962854623795\n",
      "train#4700, loss: 0.060394465923309326\n",
      "train#4800, loss: 0.6871395707130432\n",
      "train#4900, loss: 0.11106961965560913\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "!rm -rf tmp/tensorflow_log/*\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "num_of_input_nodes = 1\n",
    "num_of_hidden_nodes = 80\n",
    "num_of_output_nodes = 1\n",
    "length_of_sequences = 10\n",
    "num_of_training_epochs = 5000\n",
    "size_of_mini_batch = 100\n",
    "num_of_prediction_epochs = 100\n",
    "learning_rate = 0.01\n",
    "forget_bias = 0.8\n",
    "num_of_sample = 1000\n",
    "num_layers = 1\n",
    "\n",
    "\n",
    "def get_batch(batch_size, X, t):\n",
    "    rnum = [random.randint(0, len(X) - 1) for x in range(batch_size)]\n",
    "    xs = np.array([[[y] for y in list(X[r])] for r in rnum])\n",
    "    ts = np.array([t[r] for r in rnum])\n",
    "    return xs, ts\n",
    "\n",
    "\n",
    "def create_data(num_of_samples, sequence_len):\n",
    "    X = np.zeros((num_of_samples, sequence_len))\n",
    "    t = np.zeros((num_of_samples, 11))\n",
    "    for row_idx in range(num_of_samples):\n",
    "        X[row_idx, :] = np.random.randint(2, size=sequence_len)\n",
    "        sum = np.sum(X[row_idx, :])\n",
    "        t[row_idx, int(sum)] = 1\n",
    "    return X, t\n",
    "\n",
    "\n",
    "def unpack_sequence(tensor):\n",
    "    return tf.unpack(tf.transpose(tensor, perm=[1, 0, 2]))\n",
    "\n",
    "def pack_sequence(sequence):\n",
    "    return tf.transpose(tf.pack(sequence), perm=[1, 0, 2])\n",
    "\n",
    "def inference(input_ph):\n",
    "    with tf.name_scope(\"inference\") as scope:\n",
    "        in_size = num_of_hidden_nodes\n",
    "        out_size = 11\n",
    "        weight = tf.Variable(tf.truncated_normal([in_size, out_size], stddev=0.1))\n",
    "        bias = tf.Variable(tf.constant(0.1, shape=[out_size]))\n",
    "        \n",
    "        istate = tf.zeros((size_of_mini_batch, num_of_hidden_nodes * 2))\n",
    "        init_state = tf.Variable(istate, dtype=tf.float32, name=\"init_state\")\n",
    "       \n",
    "        network = tf.nn.rnn_cell.LSTMCell(num_of_hidden_nodes)\n",
    "        network = tf.nn.rnn_cell.DropoutWrapper(network, output_keep_prob=0.5)\n",
    "        network = tf.nn.rnn_cell.MultiRNNCell([network] * num_layers)\n",
    "        inputs =  unpack_sequence(input_ph)\n",
    "        \n",
    "        rnn_output, states_op = tf.nn.rnn(network,inputs,dtype=tf.float32)\n",
    "        #rnn_output = pack_sequence(rnn_output)\n",
    "        #state_op = pack_sequence(states_op)\n",
    "        output_op = tf.nn.softmax(tf.matmul(rnn_output[-1], weight) + bias)\n",
    "\n",
    " \n",
    "        w1_hist = tf.histogram_summary(\"weights\", weight)\n",
    "        b1_hist = tf.histogram_summary(\"biases\", bias)\n",
    "        output_hist = tf.histogram_summary(\"output\",  output_op)\n",
    "        results = [weight, bias]\n",
    "        return output_op, states_op, results\n",
    "\n",
    "\n",
    "def loss(output_op, supervisor_ph):\n",
    "    with tf.name_scope(\"loss\") as scope:\n",
    "        loss_op = - tf.reduce_sum(supervisor_ph * tf.log(output_op))\n",
    "        tf.scalar_summary(\"loss\", loss_op)\n",
    "        return loss_op\n",
    "\n",
    "\n",
    "def training(loss_op):\n",
    "    with tf.name_scope(\"training\") as scope:\n",
    "        training_op = optimizer.minimize(loss_op)\n",
    "        return training_op\n",
    "\n",
    "def accuracy(output_op, supervisor_ph):\n",
    "    with tf.name_scope(\"accuracy\") as scope:\n",
    "        correct_prediction = tf.equal(tf.argmax(output_op,1), tf.argmax(supervisor_ph,1))\n",
    "        accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        tf.scalar_summary(\"accuracy\", accuracy_op)\n",
    "        return accuracy_op\n",
    "\n",
    "def calc_accuracy(accuracy_opp, X, t):\n",
    "    inputs, targets = get_batch(len(X_train), X_train, t_train)\n",
    "    pred_dict = {\n",
    "        input_ph:  inputs,\n",
    "        supervisor_ph: targets\n",
    "    }\n",
    "    accurecy = sess.run(accuracy_op, feed_dict=pred_dict)\n",
    "    print(accurecy)\n",
    "\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "X, t = create_data(num_of_sample, length_of_sequences)\n",
    "test_num = int(num_of_sample*0.3)\n",
    "X_test = X[:test_num]\n",
    "t_test = t[:test_num]\n",
    "X_train = X[test_num:]\n",
    "t_train = t[test_num:]\n",
    "print(\"テストデータ数: \", len(X_test))\n",
    "print(\"トレーニングデータ数: \", len(X_train))\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    input_ph = tf.placeholder(tf.float32, [None, length_of_sequences, num_of_input_nodes], name=\"input\")\n",
    "    supervisor_ph = tf.placeholder(tf.float32, [None, 11], name=\"supervisor\")\n",
    "\n",
    "    output_op, states_op, datas_op = inference(input_ph)\n",
    "    loss_op = loss(output_op, supervisor_ph)\n",
    "    training_op = training(loss_op)\n",
    "    accuracy_op = accuracy(output_op, supervisor_ph)\n",
    "\n",
    "    summary_op = tf.merge_all_summaries()\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        summary_writer = tf.train.SummaryWriter(\"tmp/tensorflow_log\", graph=sess.graph)\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(num_of_training_epochs):\n",
    "            inputs, supervisors = get_batch(size_of_mini_batch, X_train, t_train)\n",
    "            train_dict = {\n",
    "                input_ph:   inputs,\n",
    "                supervisor_ph: supervisors\n",
    "            }\n",
    "            sess.run(training_op, feed_dict=train_dict)\n",
    "\n",
    "            if (epoch) % 100 == 0:\n",
    "                summary_str, train_loss = sess.run([summary_op, loss_op], feed_dict=train_dict)\n",
    "                print(\"train#{}, loss: {}\".format(epoch, train_loss))\n",
    "                summary_writer.add_summary(summary_str, epoch)\n",
    "                if (epoch) % 500 == 0:\n",
    "                    calc_accuracy(output_op, X_test, t_test)\n",
    "        calc_accuracy(output_op, X_test, t_test)\n",
    "        datas = sess.run(datas_op)\n",
    "        saver.save(sess, \"model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
