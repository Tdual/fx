{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "リスト内の1の和  \n",
    "e.g.  \n",
    "[1,1,0,0,0,0,1,0,0,0] -> 3  \n",
    "\n",
    "http://peterroelants.github.io/posts/rnn_implementation_part01/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テストデータ数:  300\n",
      "トレーニングデータ数:  700\n",
      "train#0, loss: 216.33786010742188\n",
      "0.25\n",
      "train#100, loss: 185.78494262695312\n",
      "train#200, loss: 59.8792724609375\n",
      "train#300, loss: 17.697341918945312\n",
      "train#400, loss: 49.856597900390625\n",
      "train#500, loss: 10.143465042114258\n",
      "0.99\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2a4d2e2bbef9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0msupervisor_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msupervisors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             }\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dir/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dir/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dir/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/dir/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dir/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!rm -rf tmp/tensorflow_log/*\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "num_of_input_nodes = 1\n",
    "num_of_hidden_nodes = 80\n",
    "num_of_output_nodes = 1\n",
    "length_of_sequences = 10\n",
    "num_of_training_epochs = 5000\n",
    "size_of_mini_batch = 100\n",
    "num_of_prediction_epochs = 100\n",
    "learning_rate = 0.01\n",
    "forget_bias = 0.8\n",
    "num_of_sample = 1000\n",
    "num_layers = 1\n",
    "\n",
    "\n",
    "def get_batch(batch_size, X, t):\n",
    "    rnum = [random.randint(0, len(X) - 1) for x in range(batch_size)]\n",
    "    xs = np.array([[[y] for y in list(X[r])] for r in rnum])\n",
    "    ts = np.array([t[r] for r in rnum])\n",
    "    return xs, ts\n",
    "\n",
    "\n",
    "def create_data(num_of_samples, sequence_len):\n",
    "    X = np.zeros((num_of_samples, sequence_len))\n",
    "    t = np.zeros((num_of_samples, 11))\n",
    "    for row_idx in range(num_of_samples):\n",
    "        X[row_idx, :] = np.random.randint(2, size=sequence_len)\n",
    "        sum = np.sum(X[row_idx, :])\n",
    "        t[row_idx, int(sum)] = 1\n",
    "    return X, t\n",
    "\n",
    "\n",
    "def unpack_sequence(tensor):\n",
    "    return tf.unpack(tf.transpose(tensor, perm=[1, 0, 2]))\n",
    "\n",
    "def pack_sequence(sequence):\n",
    "    return tf.transpose(tf.pack(sequence), perm=[1, 0, 2])\n",
    "\n",
    "def inference(input_ph):\n",
    "    with tf.name_scope(\"inference\") as scope:\n",
    "        in_size = num_of_hidden_nodes\n",
    "        out_size = 11\n",
    "        weight = tf.Variable(tf.truncated_normal([in_size, out_size], stddev=0.1))\n",
    "        bias = tf.Variable(tf.constant(0.1, shape=[out_size]))\n",
    "       \n",
    "        network = tf.nn.rnn_cell.LSTMCell(num_of_hidden_nodes)\n",
    "        network = tf.nn.rnn_cell.DropoutWrapper(network, output_keep_prob=0.5)\n",
    "        network = tf.nn.rnn_cell.MultiRNNCell([network] * num_layers)\n",
    "        inputs =  unpack_sequence(input_ph)\n",
    "        \n",
    "        rnn_output, states_op = tf.nn.rnn(network,inputs,dtype=tf.float32)\n",
    "        #rnn_output = pack_sequence(rnn_output)\n",
    "        #state_op = pack_sequence(states_op)\n",
    "        output_op = tf.nn.softmax(tf.matmul(rnn_output[-1], weight) + bias)\n",
    "\n",
    " \n",
    "        w1_hist = tf.histogram_summary(\"weights\", weight)\n",
    "        b1_hist = tf.histogram_summary(\"biases\", bias)\n",
    "        output_hist = tf.histogram_summary(\"output\",  output_op)\n",
    "        results = [weight, bias]\n",
    "        return output_op, states_op, results\n",
    "\n",
    "\n",
    "def loss(output_op, supervisor_ph):\n",
    "    with tf.name_scope(\"loss\") as scope:\n",
    "        loss_op = - tf.reduce_sum(supervisor_ph * tf.log(output_op))\n",
    "        tf.scalar_summary(\"loss\", loss_op)\n",
    "        return loss_op\n",
    "\n",
    "\n",
    "def training(loss_op):\n",
    "    with tf.name_scope(\"training\") as scope:\n",
    "        training_op = optimizer.minimize(loss_op)\n",
    "        return training_op\n",
    "\n",
    "def accuracy(output_op, supervisor_ph):\n",
    "    with tf.name_scope(\"accuracy\") as scope:\n",
    "        correct_prediction = tf.equal(tf.argmax(output_op,1), tf.argmax(supervisor_ph,1))\n",
    "        accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        tf.scalar_summary(\"accuracy\", accuracy_op)\n",
    "        return accuracy_op\n",
    "\n",
    "def calc_accuracy(accuracy_opp, X, t):\n",
    "    inputs, targets = get_batch(len(X), X, t)\n",
    "    pred_dict = {\n",
    "        input_ph:  inputs,\n",
    "        supervisor_ph: targets\n",
    "    }\n",
    "    accurecy = sess.run(accuracy_op, feed_dict=pred_dict)\n",
    "    print(accurecy)\n",
    "\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "X, t = create_data(num_of_sample, length_of_sequences)\n",
    "test_num = int(num_of_sample*0.3)\n",
    "X_test = X[:test_num]\n",
    "t_test = t[:test_num]\n",
    "X_train = X[test_num:]\n",
    "t_train = t[test_num:]\n",
    "print(\"テストデータ数: \", len(X_test))\n",
    "print(\"トレーニングデータ数: \", len(X_train))\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    input_ph = tf.placeholder(tf.float32, [None, length_of_sequences, num_of_input_nodes], name=\"input\")\n",
    "    supervisor_ph = tf.placeholder(tf.float32, [None, 11], name=\"supervisor\")\n",
    "\n",
    "    output_op, states_op, datas_op = inference(input_ph)\n",
    "    loss_op = loss(output_op, supervisor_ph)\n",
    "    training_op = training(loss_op)\n",
    "    accuracy_op = accuracy(output_op, supervisor_ph)\n",
    "\n",
    "    summary_op = tf.merge_all_summaries()\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        summary_writer = tf.train.SummaryWriter(\"tmp/tensorflow_log\", graph=sess.graph)\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(num_of_training_epochs):\n",
    "            inputs, supervisors = get_batch(size_of_mini_batch, X_train, t_train)\n",
    "            train_dict = {\n",
    "                input_ph:   inputs,\n",
    "                supervisor_ph: supervisors\n",
    "            }\n",
    "            sess.run(training_op, feed_dict=train_dict)\n",
    "\n",
    "            if (epoch) % 100 == 0:\n",
    "                summary_str, train_loss = sess.run([summary_op, loss_op], feed_dict=train_dict)\n",
    "                print(\"train#{}, loss: {}\".format(epoch, train_loss))\n",
    "                summary_writer.add_summary(summary_str, epoch)\n",
    "                if (epoch) % 500 == 0:\n",
    "                    calc_accuracy(output_op, X_test, t_test)\n",
    "        calc_accuracy(output_op, X_test, t_test)\n",
    "        datas = sess.run(datas_op)\n",
    "        saver.save(sess, \"model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
